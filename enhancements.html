<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>NA • fmriAR</title><script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="NA"></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">fmriAR</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><a class="dropdown-item" href="articles/fmriAR-introduction.html">Getting Started with fmriAR</a></li>
  </ul></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json"></form></li>
      </ul></div>


  </div>
</nav><div class="container template-title-body">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>NA</h1>

    </div>


<p>Here’s a prioritized roadmap to enhance your fMRI prewhitening utility while keeping the surface area small and the core blazing fast. I group items by impact/effort, include concise implementation sketches, and call out tests you can add so nothing regresses.</p>
<p>⸻</p>
<ol style="list-style-type: upper-alpha"><li>High‑impact, low‑risk (keep the API small)</li>
</ol><ol style="list-style-type: decimal"><li>Built‑in stability + selection sanity</li>
</ol><p>Why: Fewer edge‑case failures, better generalization. What: • Enforce AR stationarity and MA invertibility inside HR (root reflection or PACF‑clip), then recompute σ². • Fix BIC parameter count for AR order selection: use k = p + 1 (includes innovation variance). • Offer q=“auto” (tiny grid like q ∈ {0,1} with BIC/AICc) when p is small.</p>
<p>Tests: synthetic AR and ARMA grids; ensure selected (p,q) matches truth ≥70% on n∈[300,1000].</p>
<p>⸻</p>
<ol start="2" style="list-style-type: decimal"><li>Unique‑filter caching</li>
</ol><p>Why: Huge speed win when many parcels share (nearly) identical filters (common after multiscale pooling). What: • Hash each run’s (phi, theta) to a key (e.g., rounded to 1e‑6 + digest::digest), whiten X once per unique key, map to all parcels using that key.</p>
<p>Sketch (R):</p>
<p>hash_filter &lt;- function(phi, theta, tol = 1e-6) { paste0(round(c(phi, NA, theta), 6), collapse = “,”) } # whiten_apply(): build a map from hash -&gt; column indices, compute Xw per hash, reuse.</p>
<p>Expected: 2–10× less X‑whitening when many parcels collapse to the same plan.</p>
<p>⸻</p>
<ol start="3" style="list-style-type: decimal"><li>Deterministic threading + user knob</li>
</ol><p>Why: Reproducibility &amp; UX. What: Export set_threads(n) wrapper over the internal option; pin OMP_NUM_THREADS for whitening calls.</p>
<p>Sketch (R):</p>
<p>set_threads &lt;- function(n) { stopifnot(n &gt;= 1L) options(fmriAR.max_threads = as.integer(n)) invisible(n) }</p>
<p>Tests: the parallel determinism test you already have (exact equality across 1/2/8 threads).</p>
<p>⸻</p>
<ol start="4" style="list-style-type: decimal"><li>Parcel means: fast path by default</li>
</ol><p>Why: Multiscale estimation time becomes negligible. What: Use your parcel_means_cpp() everywhere. Keep R fallback behind an option (you already have this).</p>
<p>Tests: verify equality to R implementation on random small inputs; micro‑benchmark.</p>
<p>⸻</p>
<ol start="5" style="list-style-type: decimal"><li>Whiteness QA helpers (S3)</li>
</ol><p>Why: Users need a one‑liner to validate plans. What: Add check_whiteness(plan, X, Y, run_starts, lag=10) that returns: • KS distance to Uniform for Ljung–Box p‑values • fraction p≤0.05 • lag‑wise ACF energy</p>
<p>Expose plot() method (base or ggplot2) without adding heavy deps by making ggplot2 Suggests.</p>
<p>Tests: your “multiscale improves KS/NLL” tests double as acceptance tests.</p>
<p>⸻</p>
<ol start="6" style="list-style-type: decimal"><li>Robust SE variants &amp; contrasts</li>
</ol><p>Why: Downstream inference benefits. What: Extend sandwich_from_whitened_resid() to support type = c(“iid”,“hc0”,“hc1”,“hc2”,“hc3”) and a contrast(beta, L) helper.</p>
<p>Sketch (R):</p>
<p>sandwich_from_whitened_resid &lt;- function(Xw, ew, type=c(“iid”,“hc0”,“hc1”,“hc2”,“hc3”)) { … } contrast &lt;- function(beta, Sigma_beta, L) { est &lt;- as.numeric(L %<em>% beta); var &lt;- L %</em>% Sigma_beta %*% t(L) list(estimate=est, se=sqrt(diag(var)), Sigma=var) }</p>
<p>Tests: HC3 ≥ HC0 elementwise on heteroskedastic sims; contrasts match closed‑form on simple designs.</p>
<p>⸻</p>
<ol start="2" style="list-style-type: upper-alpha"><li>Modeling capabilities (medium effort, big value)</li>
</ol><ol start="7" style="list-style-type: decimal"><li>Kalman innovations (exact ARMA, missing data tolerant)</li>
</ol><p>Why: Exact likelihood and innovations for ARMA; handles censored samples and arbitrary segmenting without ad‑hoc resets. What: Add a kalman_whiten_inplace() (state‑space for ARMA p,q) used when missing or q&gt;0 with complex segment patterns; fallback to current fast loop for pure AR or dense data.</p>
<p>Implementation notes: • Build companion form for AR(p), augment with MA(q) to innovations form; use steady‑state Kalman when possible; otherwise per‑segment reinit. • Keep identical API; select engine internally.</p>
<p>Tests: missing points per voxel; compare to dense case where NA dropped → identical innovations on overlapping indices.</p>
<p>⸻</p>
<ol start="8" style="list-style-type: decimal"><li>Local (slowly varying) AR: piecewise‑stationary</li>
</ol><p>Why: Nonstationarities across long runs (drift, vigilance). What: Optional sliding‑window AR with overlap (e.g., windows of 60–120 TRs, 50% overlap), smooth φ across windows (e.g., quadratic penalty), and whiten using time‑varying filter (change points handled at window edges with small cross‑fade).</p>
<p>API idea: fit_noise(…, local_ar = list(window=90, overlap=0.5, penalty=“quad”)).</p>
<p>Tests: simulate φ(t) drift; multiscale+local reduces ACF energy vs static AR.</p>
<p>⸻</p>
<ol start="9" style="list-style-type: decimal"><li>Empirical‑Bayes weight learning for multiscale</li>
</ol><p>Why: Replace hand‑tuned weights with data‑driven shrinkage strength. What: Work in PACF‑space: use z = atanh(κ) (Gaussianizable); put a normal prior centered at parent z with variance τ²; estimate τ² per lag (and per level) by EB (marginal likelihood or SURE). The posterior mean becomes the shrinkage target.</p>
<p>Benefit: principled control of shrinkage; adapts across datasets.</p>
<p>Tests: on hierarchical sims, EB‑weights should beat fixed weights on held‑out KS/NLL.</p>
<p>⸻</p>
<ol start="10" style="list-style-type: decimal"><li>Physio‑aware nuisance templates (optional input)</li>
</ol><p>Why: Better residuals when physio traces exist. What: Provide a tiny helper to ingest fMRIPrep confounds or RETROICOR regressors; nothing fancy—just a convenience build_confounds() that returns an X with recommended sels.</p>
<p>Tests: presence/absence yields improved whiteness on physio‑simulated data.</p>
<p>⸻</p>
<ol start="3" style="list-style-type: upper-alpha"><li>Performance &amp; scale</li>
</ol><ol start="11" style="list-style-type: decimal"><li>Streaming whitening (chunked)</li>
</ol><p>Why: Reduce peak memory &amp; enable very long runs. What: Process Y in chunks by time with overlap L = max(p,q); maintain per‑column state across chunks.</p>
<p>API: whiten_apply(…, stream_by = 2000).</p>
<p>Tests: stream vs full → identical outputs (within machine epsilon).</p>
<p>⸻</p>
<ol start="12" style="list-style-type: decimal"><li>SIMD‑friendly inner loops</li>
</ol><p>Why: Small but real speedups, especially AR(1–2). What: • Add #pragma omp simd on the time loop; • Provide specialized kernels for p=1 and p=2 (branch once per column).</p>
<p>Sketch (C++):</p>
<p>if (p==1 &amp;&amp; q==0) whiten_ar1(colPtr, n, phi0, segs); else if (p==2 &amp;&amp; q==0) whiten_ar2(colPtr, n, phi0, phi1, segs); else whiten_arma_generic(…);</p>
<p>Tests: speed micro‑benchmarks; outputs identical.</p>
<p>⸻</p>
<ol start="13" style="list-style-type: decimal"><li>Memoize filtered design blocks</li>
</ol><p>Why: X often repeats across subjects/runs; avoid recomputation across calls. What: Option to cache Xw per (design_hash, filter_hash, run) in a user‑provided cache env.</p>
<p>API: whiten_apply(…, cache = new.env()).</p>
<p>Tests: repeated calls reuse cache (instrument counters).</p>
<p>⸻</p>
<ol start="4" style="list-style-type: upper-alpha"><li>Developer experience &amp; reliability</li>
</ol><ol start="14" style="list-style-type: decimal"><li>Plan I/O &amp; reproducibility</li>
</ol><p>Why: Shareable, cached plans across sessions. What: save_plan(plan, file) / load_plan(file) using jsonlite; include version, p/q, φ/θ by parcel, weights, order rule, options.</p>
<p>Tests: round‑trip equality and compatibility across minor versions.</p>
<p>⸻</p>
<ol start="15" style="list-style-type: decimal"><li>Vignettes &amp; autoplot</li>
</ol><p>Why: Onboarding &amp; trust. What: • “Prewhitening for fMRI in practice” (why, how, diagnostics). • “Multiscale pooling: when it helps and when it doesn’t.” • “Exact ARMA via Kalman with missing data.”</p>
<p>⸻</p>
<ol start="16" style="list-style-type: decimal"><li>CI + sanitizers</li>
</ol><p>Why: Catch UB and threading bugs early. What: GH Actions matrix: Linux/macOS, R release/devel; run ASan/UBSan builds for C++; run testthat with OMP_NUM_THREADS=1,2,8.</p>
<p>⸻</p>
<ol start="17" style="list-style-type: decimal"><li>Configuration for OpenMP portability</li>
</ol><p>Why: CRAN friendliness. What: Autodetect OMP; if absent, compile a serial fallback; expose capabilities() helper to show threading status.</p>
<p>⸻</p>
<ol start="5" style="list-style-type: upper-alpha"><li>Tiny polish (nice to have) • print() / summary() for fmriAR_plan (show (p,q), #unique filters, weights summary, KS from training). • Better errors on invalid run_starts: echo first bad index and expectations (strictly increasing, 0‑based, includes 0). • Progress reporting (optional): use progressr in R; no C++ calls from worker threads.</li>
</ol><p>⸻</p>
<p>Concrete code snippets you can drop in</p>
<p>A robust OLS fallback in HR (prevents singularity issues)</p>
<p>arma::vec coef; bool ok = arma::solve(coef, Z, ysub, arma::solve_opts::fast); if (!ok || coef.n_elem != Z.n_cols) { ok = arma::solve(coef, Z, ysub); // QR/LAPACK fallback } if (!ok) return hr_failure(p, q, p_big, iter);</p>
<p>AR(1) specialized kernel (illustrative)</p>
<p>inline void whiten_ar1(double* y, int n, double phi0, const int* seg_beg, const int* seg_end, int nseg, double scale_first) { for (int s=0; s&lt;nseg; ++s) { int a = seg_beg[s], b = seg_end[s]; double ym1 = 0.0, em1 = 0.0; // not used for pure AR if (a &lt; b) { double e = (y[a] - phi0 * ym1) * scale_first; ym1 = y[a]; y[a] = e; for (int t=a+1; t&lt;b; ++t) { double st = y[t] - phi0 * ym1; ym1 = y[t]; y[t] = st; } } } }</p>
<p>set_threads() + capability probe</p>
<p>set_threads &lt;- function(n) { stopifnot(n &gt;= 1L) options(fmriAR.max_threads = as.integer(n)) invisible(n) } capabilities_fmriAR &lt;- function() { list(openmp = fmriAR:::omp_capable(), # C++ tiny function returning bool simd = TRUE) # if you add omp simd, toggle here }</p>
<p>Plan I/O</p>
<p>save_plan &lt;- function(plan, file) { jsonlite::write_json(fmriAR:::compat<span class="math inline">$plan_info(plan), path = file, auto_unbox = TRUE, pretty = TRUE)
}
load_plan &lt;- function(file) {
  info &lt;- jsonlite::read_json(file, simplifyVector = TRUE)
  fmriAR:::compat$</span>plan_from_phi(info<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>h</mi><msub><mi>i</mi><mi>b</mi></msub><msub><mi>y</mi><mi>p</mi></msub><mi>a</mi><mi>r</mi><mi>c</mi><mi>e</mi><mi>l</mi><mo>,</mo><mi>i</mi><mi>n</mi><mi>f</mi><mi>o</mi></mrow><annotation encoding="application/x-tex">phi_by_parcel, info</annotation></semantics></math>p, info<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo>,</mo><mi>i</mi><mi>n</mi><mi>f</mi><mi>o</mi></mrow><annotation encoding="application/x-tex">q, info</annotation></semantics></math>options) }</p>
<p>⸻</p>
<p>Suggested acceptance tests (add to tests/testthat/) • EB multiscale vs fixed: EB weights must not regress KS/NLL relative to fixed weights on 3 synthetic seeds. • Kalman vs generic: For data without missingness, Kalman innovations ≈ generic innovations (1e‑12). • Streaming vs full: Chunked whitening equals full whitening (1e‑12) on random data with multiple segments. • Cache hit rate: With duplicated filters across parcels, number of X‑whitenings is reduced exactly to #unique_filters.</p>
<p>⸻</p>
<p>Summary</p>
<p>You can keep the interface tiny and still make the package feel “complete” by: • hardening estimation (stability + selection), • cutting repeated work (filter caching, streaming), • offering one exact method for tricky cases (Kalman), • giving users first‑class diagnostics and an easy way to reproduce/share plans.</p>
<p>If you want, I can prep a branch layout (feature flags, files to touch) and a minimal bench + QA vignette skeleton to drop into vignettes/.</p>


  </main></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Your Name.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer></div>





  </body></html>

